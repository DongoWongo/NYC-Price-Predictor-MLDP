{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fa6d7f",
   "metadata": {},
   "source": [
    "# Declaration of Originality"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "58ad86aa",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "**TEMASEK POLYTECHNIC**\n",
    "<br>**SCHOOL OF INFORMATICS & IT**\n",
    "<br>**DIPLOMA IN INFORMATION TECHNOLOGY**\n",
    "<br>**MACHINE LEARNING FOR DEVELOPERS (CAI2C08)**\n",
    "<br>**AY2025/2026 OCTOBER SEMESTER**\n",
    "\n",
    "**PROJECT PROGRAM CODES**\n",
    "* Student Name (Matric Number)  : Wong Yong Xiang (2401613A)\n",
    "* Tutorial Group                : P01\n",
    "* Tutor\t\t\t\t\t\t    : Ruchir SRIVASTAVA\n",
    "* Submission Date               : 11/2/2026\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f1aba",
   "metadata": {},
   "source": [
    "**Declaration of Originality**\n",
    "* I am the originator of this work and I have appropriately acknowledged all other original sources used as my references for this work.\n",
    "* I understand that Plagiarism is the act of taking and using the whole or any part of another personâ€™s work, including work generated by AI, and presenting it as my own.\n",
    "* I understand that Plagiarism is an academic offence and if I am found to have committed or abetted the offence of plagiarism in relation to this submitted work, disciplinary action will be enforced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd708a7b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### NYC Real Estate Sale Price Prediction\n",
    "\n",
    "Objective: Predict sale price of residential properties in New York City using property characteristics, location data, and building features.\n",
    "\n",
    "Task Type: Regression (continuous numerical target, sale price in USD)\n",
    "\n",
    "Models: Ridge Regression, Random Forest Regressor, Histogram-based Gradient Boosting Regressor\n",
    "\n",
    "---\n",
    "### 1. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2273e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.2f}')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93490d6",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Data Loading and Exploration\n",
    "\n",
    "Source: NYC Department of Finance Rolling Sales Data (2016-2017), obtained from Kaggle.\n",
    "The dataset contains all property sale transactions across all five NYC boroughs over a 12-months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f49d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('nyc-rolling-sales.csv')\n",
    "print(f\"Dataset shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "print(f\"\\nColumn names:\\n{list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53877515",
   "metadata": {},
   "source": [
    "### Data type observations\n",
    "\n",
    "Several numeric columns are stored as strings in the raw CSV and need conversion:\n",
    "- SALE PRICE contains commas and dashes (non-market entries)\n",
    "- GROSS SQUARE FEET and LAND SQUARE FEET contain dashes for missing values\n",
    "- Unnamed: 0 is a CSV row index artefact and it'll be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61af03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMissing values (top 10):\")\n",
    "print(df.isnull().sum().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6ee0ad",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.1 Exploratory Data Analysis\n",
    "\n",
    "Below is to examine target variable distribution and feature relationships before cleaning. This is so we don't change any important data that might have important relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1a6f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string columns to numeric for initial exploration\n",
    "df['SALE PRICE'] = pd.to_numeric(df['SALE PRICE'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "df['GROSS SQUARE FEET'] = pd.to_numeric(df['GROSS SQUARE FEET'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "df['LAND SQUARE FEET'] = pd.to_numeric(df['LAND SQUARE FEET'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "\n",
    "print(f\"Sale price stats (raw, before cleaning):\")\n",
    "print(df['SALE PRICE'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb0a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw distribution\n",
    "valid_prices = df['SALE PRICE'][(df['SALE PRICE'] > 0) & (df['SALE PRICE'] < df['SALE PRICE'].quantile(0.99))]\n",
    "axes[0].hist(valid_prices, bins=80, color='steelblue', edgecolor='white', alpha=0.85)\n",
    "axes[0].set_title('Raw Sale Price Distribution (< 99th pctile)')\n",
    "axes[0].set_xlabel('Sale Price (USD)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'${x/1e6:.1f}M'))\n",
    "\n",
    "# Log distribution\n",
    "valid_log = np.log(df['SALE PRICE'][df['SALE PRICE'] > 10000])\n",
    "axes[1].hist(valid_log, bins=80, color='darkorange', edgecolor='white', alpha=0.85)\n",
    "axes[1].set_title('Log(Sale Price) Distribution (> $10K)')\n",
    "axes[1].set_xlabel('Log(Sale Price)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eed3ff0",
   "metadata": {},
   "source": [
    "### Target distribution analysis\n",
    "\n",
    "The raw sale price distribution is heavily skewed to the right, with a long tail of luxury properties reaching into the billions. After log-transformation, the distribution becomes approximately normal, which is better for regression modelling.\n",
    "\n",
    "We will use log(SALE PRICE) as our training target. This will:\n",
    "1. Normalise the skewed distribution\n",
    "2. Reduce the influence of extreme outliers\n",
    "3. Makes percentage errors more uniform across price ranges\n",
    "4. Predictions are transformed back with exp() for dollar-scale evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d5385",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Borough sale counts\n",
    "borough_map = {1: 'Manhattan', 2: 'Bronx', 3: 'Brooklyn', 4: 'Queens', 5: 'Staten Island'}\n",
    "borough_counts = df['BOROUGH'].map(borough_map).value_counts()\n",
    "borough_counts.plot(kind='bar', ax=axes[0], color='steelblue', edgecolor='white')\n",
    "axes[0].set_title('Sales by Borough')\n",
    "axes[0].set_ylabel('Number of Sales')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Borough median prices (valid sales only)\n",
    "valid_sales = df[df['SALE PRICE'] > 10000].copy()\n",
    "valid_sales['BOROUGH_NAME'] = valid_sales['BOROUGH'].map(borough_map)\n",
    "borough_medians = valid_sales.groupby('BOROUGH_NAME')['SALE PRICE'].median().sort_values(ascending=False)\n",
    "borough_medians.plot(kind='bar', ax=axes[1], color='darkorange', edgecolor='white')\n",
    "axes[1].set_title('Median Sale Price by Borough (> $10K)')\n",
    "axes[1].set_ylabel('Median Price (USD)')\n",
    "axes[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'${x/1e6:.1f}M'))\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27043ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gross sqft vs price\n",
    "mask = (df['SALE PRICE'] > 10000) & (df['GROSS SQUARE FEET'] > 0) & (df['SALE PRICE'] < df['SALE PRICE'].quantile(0.99))\n",
    "axes[0].scatter(df.loc[mask, 'GROSS SQUARE FEET'], df.loc[mask, 'SALE PRICE'], alpha=0.15, s=5, color='steelblue')\n",
    "axes[0].set_title('Gross Square Feet vs Sale Price')\n",
    "axes[0].set_xlabel('Gross Square Feet')\n",
    "axes[0].set_ylabel('Sale Price (USD)')\n",
    "axes[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'${x/1e6:.1f}M'))\n",
    "\n",
    "# Year built vs price\n",
    "mask2 = (df['SALE PRICE'] > 10000) & (df['YEAR BUILT'] > 0) & (df['SALE PRICE'] < df['SALE PRICE'].quantile(0.99))\n",
    "axes[1].scatter(df.loc[mask2, 'YEAR BUILT'], df.loc[mask2, 'SALE PRICE'], alpha=0.15, s=5, color='darkorange')\n",
    "axes[1].set_title('Year Built vs Sale Price')\n",
    "axes[1].set_xlabel('Year Built')\n",
    "axes[1].set_ylabel('Sale Price (USD)')\n",
    "axes[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'${x/1e6:.1f}M'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa9d183",
   "metadata": {},
   "source": [
    "### EDA summary\n",
    "\n",
    "| Observation | Conclusion |\n",
    "|---|---|\n",
    "| Manhattan median price is 3-5x outer boroughs | Location (borough/neighbourhood) is the strongest predictor |\n",
    "| Queens has the most transactions (31%) | Model will have most training signal for Queens properties |\n",
    "| Gross sqft shows positive but noisy correlation with price | Size matters but location modulates the relationship |\n",
    "| Year built shows weak direct correlation | Need to engineer building_age and interact with location |\n",
    "| Many $0 and $1 sales exist in raw data | Non-market transfers, need to filter by $10K threshold maybe |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097e655a",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Data Cleaning and Preparation\n",
    "\n",
    "1. **Drop Unnamed: 0** - CSV row index artefact with no predictive value; would introduce spurious correlations\n",
    "2. **Remove non-market sales (SALE PRICE <= $10,000)** - Transactions at $0, $1, or $10 represent things like intra-family transfers, also mentioned in the dataset overview, not true market prices\n",
    "3. **Cap outliers at 99th percentile** - Extreme luxury sales (up to $2.2B) would dominate RMSE and mess up model training; winsorising will mostly retain rank order while reducing their impact\n",
    "4. **Replace YEAR BUILT = 0 with NaN** - Zero is a placeholder for missing construction date, not a valid year; keeping it would create misleading age calculations\n",
    "5. **Replace GROSS/LAND SQUARE FEET = 0 with NaN** - Zero square footage indicates missing data, not properties with 0 space, obviously removed\n",
    "6. **Remove rows where YEAR BUILT is still missing** - Building age is an important engineered feature, rows without it must be removed\n",
    "7. **Drop low-information columns** - ADDRESS, APARTMENT NUMBER (too granular), EASE-MENT (no variance), BUILDING CLASS AT PRESENT/TIME OF SALE (redundant with CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_count = len(df)\n",
    "\n",
    "# Step 1: Drop CSV index artefact\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Step 2: Remove non-market sales\n",
    "df = df[df['SALE PRICE'] > 10000].copy()\n",
    "print(f\"After removing non-market sales (<= $10K): {len(df):,} rows (removed {initial_count - len(df):,})\")\n",
    "\n",
    "# Step 3: Cap outliers at 99th percentile\n",
    "cap = df['SALE PRICE'].quantile(0.99)\n",
    "df['SALE PRICE'] = df['SALE PRICE'].clip(upper=cap)\n",
    "print(f\"Sale price capped at 99th percentile: ${cap:,.0f}\")\n",
    "\n",
    "# Step 4-5: Replace zero values with NaN\n",
    "df['YEAR BUILT'] = df['YEAR BUILT'].replace(0, np.nan)\n",
    "df['GROSS SQUARE FEET'] = df['GROSS SQUARE FEET'].replace(0, np.nan)\n",
    "df['LAND SQUARE FEET'] = df['LAND SQUARE FEET'].replace(0, np.nan)\n",
    "\n",
    "# Step 6: Remove rows with missing YEAR BUILT\n",
    "before_yb = len(df)\n",
    "df = df.dropna(subset=['YEAR BUILT'])\n",
    "print(f\"After removing missing YEAR BUILT: {len(df):,} rows (removed {before_yb - len(df):,})\")\n",
    "\n",
    "# Step 7: Drop low-information columns\n",
    "drop_cols = ['ADDRESS', 'APARTMENT NUMBER', 'EASE-MENT',\n",
    "             'BUILDING CLASS AT PRESENT', 'BUILDING CLASS AT TIME OF SALE']\n",
    "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "print(f\"\\nFinal cleaned dataset: {len(df):,} rows x {len(df.columns)} columns\")\n",
    "print(f\"Retention rate: {len(df)/initial_count*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce3b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify cleaning results\n",
    "print(\"Remaining columns:\", list(df.columns))\n",
    "print(f\"\\nSale price range: ${df['SALE PRICE'].min():,.0f} -- ${df['SALE PRICE'].max():,.0f}\")\n",
    "print(f\"Missing values after cleaning:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a20c9",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.1 Feature Engineering\n",
    "\n",
    "Engineered features capture real world knowledge about real estate pricing that raw columns cannot show directly.\n",
    "\n",
    "| Feature | Formula | Reason |\n",
    "|---|---|---|\n",
    "| building_age | 2017 - YEAR BUILT | Older buildings may depreciate or appreciate (historic value); more intuitive than raw year |\n",
    "| age_squared | building_age^2 | Captures non-linear age effects (very old buildings can be valuable historic properties) |\n",
    "| is_manhattan | BOROUGH = 1 | Manhattan has fundamentally different pricing dynamics; binary flag simplifies learning |\n",
    "| has_commercial | COMMERCIAL UNITS > 0 | Mixed-use properties command different valuations |\n",
    "| is_multi_unit | TOTAL UNITS > 1 | Multi-family buildings priced differently than single-family |\n",
    "| sale_month | Extracted from SALE DATE | Seasonal patterns in real estate (spring/summer premium) |\n",
    "| sale_quarter | Extracted from SALE DATE | Broader seasonal grouping |\n",
    "| log_gross_sqft | log(GROSS SQUARE FEET) | Diminishing marginal value of additional space; handles skew |\n",
    "| neigh_median_price | Neighbourhood group median | Location quality proxy - captures desirability at fine granularity |\n",
    "| zip_median_price | ZIP code group median | Additional location signal at postal-code level |\n",
    "| borough_freq / neigh_freq / zip_freq | Frequency encoding | Converts high-cardinality categoricals to numeric without explosion of dimensions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43fe7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal features\n",
    "df['SALE DATE'] = pd.to_datetime(df['SALE DATE'])\n",
    "df['sale_month'] = df['SALE DATE'].dt.month\n",
    "df['sale_quarter'] = df['SALE DATE'].dt.quarter\n",
    "\n",
    "# Building age features (capped at 200 years to prevent extreme outliers)\n",
    "df['building_age'] = (2017 - df['YEAR BUILT']).clip(upper=200)\n",
    "df['age_squared'] = df['building_age'] ** 2\n",
    "\n",
    "# Cap extreme unit counts at 99th percentile to prevent Ridge extrapolation issues\n",
    "for col in ['RESIDENTIAL UNITS', 'COMMERCIAL UNITS', 'TOTAL UNITS']:\n",
    "    cap_val = df[col].quantile(0.99)\n",
    "    df[col] = df[col].clip(upper=cap_val)\n",
    "\n",
    "# Binary indicators\n",
    "df['is_manhattan'] = (df['BOROUGH'] == 1).astype(int)\n",
    "df['has_commercial'] = (df['COMMERCIAL UNITS'] > 0).astype(int)\n",
    "df['is_multi_unit'] = (df['TOTAL UNITS'] > 1).astype(int)\n",
    "\n",
    "# Log-transform of square footage (NaN-safe)\n",
    "df['log_gross_sqft'] = np.log1p(df['GROSS SQUARE FEET'])\n",
    "\n",
    "# Location-based aggregations (median price by neighbourhood and ZIP)\n",
    "df['neigh_median_price'] = df.groupby('NEIGHBORHOOD')['SALE PRICE'].transform('median')\n",
    "df['zip_median_price'] = df.groupby('ZIP CODE')['SALE PRICE'].transform('median')\n",
    "\n",
    "# Frequency encoding for high-cardinality categoricals\n",
    "for col in ['BOROUGH', 'NEIGHBORHOOD', 'ZIP CODE', 'BUILDING CLASS CATEGORY',\n",
    "            'TAX CLASS AT PRESENT', 'TAX CLASS AT TIME OF SALE']:\n",
    "    df[col.lower().replace(' ', '_') + '_freq'] = df[col].map(df[col].value_counts(normalize=True))\n",
    "\n",
    "print(f\"Features after engineering: {len(df.columns)} columns\")\n",
    "print(f\"Engineered features: building_age, age_squared, is_manhattan, has_commercial,\")\n",
    "print(f\"  is_multi_unit, sale_month, sale_quarter, log_gross_sqft,\")\n",
    "print(f\"  neigh_median_price, zip_median_price, + 6 frequency-encoded columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef49c7",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2 Correlation Analysis and Feature Selection\n",
    "\n",
    "Now to examine feature correlations with the target and between features:\n",
    "- To confirm that engineered features have predictive signal\n",
    "- To identify and remove redundant features (correlation > 0.95)\n",
    "- To select the final feature set for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157dd384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final feature matrix\n",
    "drop_for_model = ['SALE PRICE', 'SALE DATE', 'NEIGHBORHOOD', 'BUILDING CLASS CATEGORY',\n",
    "                   'TAX CLASS AT PRESENT', 'TAX CLASS AT TIME OF SALE', 'ZIP CODE',\n",
    "                   'YEAR BUILT', 'BLOCK', 'LOT']\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in drop_for_model]\n",
    "print(f\"Feature set: {len(feature_cols)} features\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "\n",
    "# Correlation with target (log price)\n",
    "log_price = np.log(df['SALE PRICE'])\n",
    "correlations = df[feature_cols].corrwith(log_price).abs().sort_values(ascending=False)\n",
    "print(f\"\\nTop 10 features correlated with log(SALE PRICE):\")\n",
    "print(correlations.head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7bd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap of numeric features\n",
    "numeric_feats = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "corr_matrix = df[numeric_feats].corr()\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', center=0,\n",
    "            linewidths=0.5, vmin=-1, vmax=1)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Flag highly correlated pairs\n",
    "high_corr = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.95:\n",
    "            high_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "if high_corr:\n",
    "    print(\"Highly correlated feature pairs (|r| > 0.95):\")\n",
    "    for f1, f2, r in high_corr:\n",
    "        print(f\"  {f1} <-> {f2}: {r:.3f}\")\n",
    "else:\n",
    "    print(\"No feature pairs with |r| > 0.95\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b8f28e",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Train-Test Split and Data Preparation\n",
    "\n",
    "Split ratio: 80% train / 20% test, with random_state=42 for reproducibility.\n",
    "\n",
    "Target transformation: y = log(SALE PRICE). All models are trained on log-transformed prices and evaluated on the dollar scale after inverse transformation (exp()).\n",
    "\n",
    "NaN handling:\n",
    "- HistGradientBoostingRegressor handles NaN values natively (built-in support for missing data), so it receives the original feature matrix with NaN intact\n",
    "- Ridge Regression and Random Forest require complete data, so they receive a median-imputed copy where medians are computed from the training set only (preventing data leakage)\n",
    "- StandardScaler is applied only for Ridge Regression (regularisation is sensitive to feature scale); tree-based models are scale-invariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feature_cols].select_dtypes(include=[np.number])\n",
    "y = np.log(df['SALE PRICE'])  # Log-transformed target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Test set:     {X_test.shape[0]:,} samples, {X_test.shape[1]} features\")\n",
    "print(f\"\\nTarget (log scale) -- Train mean: {y_train.mean():.3f}, Test mean: {y_test.mean():.3f}\")\n",
    "\n",
    "# Median imputation from training set (for RF and Ridge)\n",
    "impute_medians = X_train.median()\n",
    "X_train_imputed = X_train.fillna(impute_medians)\n",
    "X_test_imputed = X_test.fillna(impute_medians)\n",
    "\n",
    "# Standard scaling for Ridge (tree models do not need this)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imputed),\n",
    "                               columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_imputed),\n",
    "                              columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(f\"\\nNaN counts in training features: {X_train.isnull().sum().sum()} (used by HistGBR)\")\n",
    "print(f\"NaN counts after imputation: {X_train_imputed.isnull().sum().sum()} (used by RF, Ridge)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5936bbdf",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.1 Model Selection and Training\n",
    "\n",
    "Model 1: Ridge Regression (Baseline)\n",
    "- `sklearn.linear_model.Ridge`\n",
    "- Linear model with L2 regularisation to handle correlated features (borough_freq, neigh_freq, zip_freq are all location signals)\n",
    "- Provides interpretable coefficients showing which features drive price\n",
    "- Expected to underfit because real estate pricing is inherently non-linear: a 2,000 sqft property in Manhattan has much higher premium than the same size in the Bronx, which linear models cannot fully capture\n",
    "- Serves as the performance floor against which tree-based models are measured\n",
    "\n",
    "Model 2: Random Forest Regressor\n",
    "- `sklearn.ensemble.RandomForestRegressor`\n",
    "- Ensemble of decision trees using bagging (bootstrap aggregation) to reduce variance\n",
    "- Naturally captures non-linear relationships (e.g., neighbourhood-specific price-per-sqft dynamics) and feature interactions without explicit engineering\n",
    "- Provides feature importance rankings useful for stakeholder communication\n",
    "- Robust to outliers due to median-based splits in constituent trees\n",
    "- The 50K+ training samples provide sufficient data for the trees to learn complex patterns without overfitting\n",
    "\n",
    "Model 3: Histogram-based Gradient Boosting Regressor\n",
    "- `sklearn.ensemble.HistGradientBoostingRegressor`\n",
    "- scikit-learn's modern gradient boosting implementation, comparable to XGBoost, which unfortunately, is not allowed since outside of sklearn\n",
    "- Sequential boosting: each tree corrects the errors of the previous ensemble, progressively refining predictions\n",
    "- Histogram binning efficiently handles the high-cardinality frequency-encoded features\n",
    "- Native NaN support means it can learn patterns from other features when square footage is missing, rather than relying on imputed values\n",
    "- Built-in L2 regularisation and early stopping prevent overfitting despite model complexity\n",
    "- Expected to achieve the best performance on this tabular regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd64fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation helper -- computes metrics on the dollar scale\n",
    "def evaluate_model(y_true_log, y_pred_log, set_name=''):\n",
    "    \"\"\"Evaluate predictions by transforming back to dollar scale.\n",
    "    Clips log-space predictions to [5, 25] to prevent numerical overflow\n",
    "    when exponentiating (corresponds to ~$150 - ~$72B in dollar space).\n",
    "    \"\"\"\n",
    "    y_pred_log = np.clip(y_pred_log, 5, 25)\n",
    "    y_true = np.exp(y_true_log)\n",
    "    y_pred = np.exp(y_pred_log)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print(f\"  {set_name + ':' if set_name else '':<12} R2={r2:.4f}  RMSE=${rmse:,.0f}  MAE=${mae:,.0f}  MAPE={mape:.1f}%\")\n",
    "    return {'R2': r2, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd21c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# --- Ridge Regression ---\n",
    "print(\"Ridge Regression\")\n",
    "ridge = Ridge(alpha=1.0, random_state=42)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "results['Ridge'] = {\n",
    "    'train': evaluate_model(y_train, ridge.predict(X_train_scaled), 'Train'),\n",
    "    'test': evaluate_model(y_test, ridge.predict(X_test_scaled), 'Test')\n",
    "}\n",
    "\n",
    "print()\n",
    "\n",
    "# --- Random Forest ---\n",
    "print(\"Random Forest\")\n",
    "rf = RandomForestRegressor(n_estimators=200, max_depth=20, min_samples_leaf=2,\n",
    "                           random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_imputed, y_train)\n",
    "results['RandomForest'] = {\n",
    "    'train': evaluate_model(y_train, rf.predict(X_train_imputed), 'Train'),\n",
    "    'test': evaluate_model(y_test, rf.predict(X_test_imputed), 'Test')\n",
    "}\n",
    "\n",
    "print()\n",
    "\n",
    "# --- HistGradientBoosting ---\n",
    "print(\"HistGradientBoosting\")\n",
    "hgb = HistGradientBoostingRegressor(max_iter=200, learning_rate=0.05, max_depth=10,\n",
    "                                     min_samples_leaf=20, random_state=42)\n",
    "hgb.fit(X_train, y_train)\n",
    "results['HistGBR'] = {\n",
    "    'train': evaluate_model(y_train, hgb.predict(X_train), 'Train'),\n",
    "    'test': evaluate_model(y_test, hgb.predict(X_test), 'Test')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f261b62",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.2 Feature Importance Analysis\n",
    "\n",
    "Feature importance from Random Forest and HistGradientBoosting shows which property attributes most influence sale price. This provides interpretability for stakeholders and validates that the model is learning sensible patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d28b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Random Forest feature importance\n",
    "rf_imp = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=True)\n",
    "rf_imp.tail(15).plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Random Forest -- Top 15 Features')\n",
    "axes[0].set_xlabel('Importance')\n",
    "\n",
    "# HistGBR permutation importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "perm_imp = permutation_importance(hgb, X_test, y_test, n_repeats=5, random_state=42, n_jobs=-1)\n",
    "hgb_imp = pd.Series(perm_imp.importances_mean, index=X_train.columns).sort_values(ascending=True)\n",
    "hgb_imp.tail(15).plot(kind='barh', ax=axes[1], color='darkorange')\n",
    "axes[1].set_title('HistGradientBoosting -- Top 15 Features (Permutation)')\n",
    "axes[1].set_xlabel('Mean Importance Decrease')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10469bf7",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.3 Cross-Validation\n",
    "\n",
    "5-fold cross-validation provides a better estimate of model performance than a single train-test split, by averaging over multiple random partitions. This helps confirm that our results are not due to a lucky split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fed672",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"5-Fold Cross-Validation (scoring = neg_mean_squared_error on log scale):\")\n",
    "print()\n",
    "\n",
    "for name, model, X_cv in [('Ridge', Ridge(alpha=1.0, random_state=42), X_train_scaled),\n",
    "                           ('RandomForest', RandomForestRegressor(n_estimators=200, max_depth=20,\n",
    "                                                                    min_samples_leaf=2, random_state=42, n_jobs=-1), X_train_imputed),\n",
    "                           ('HistGBR', HistGradientBoostingRegressor(max_iter=200, learning_rate=0.05,\n",
    "                                                                       max_depth=10, min_samples_leaf=20, random_state=42), X_train)]:\n",
    "    scores = cross_val_score(model, X_cv, y_train, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    rmse_scores = np.sqrt(-scores)\n",
    "    print(f\"  {name:<16} Log-RMSE: {rmse_scores.mean():.4f} +/- {rmse_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd91f2",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Iterative Model Improvement: Hyperparameter Tuning\n",
    "\n",
    "RandomizedSearchCV with 5-fold cross-validation and n_iter=25 random configurations per model.\n",
    "\n",
    "Maximum 3 values per hyperparameter as per proj requirement.\n",
    "\n",
    "| Model | Hyperparameter | Values | Rationale |\n",
    "|---|---|---|---|\n",
    "| **Ridge** | alpha | 0.1, 1.0, 10.0 | Controls regularisation strength, logarithmic spacing covers weak to strong penalty |\n",
    "| **Random Forest** | n_estimators | 100, 200, 300 | More trees reduce variance, diminishing returns beyond 300 |\n",
    "| | max_depth | 10, 20, None | Controls tree complexity, none allows full growth |\n",
    "| | min_samples_split | 2, 5, 10 | Higher values regularise by requiring more data to split |\n",
    "| | min_samples_leaf | 1, 2, 4 | Minimum leaf size acts as regularisation |\n",
    "| **HistGBR** | max_iter | 100, 200, 300 | Number of boosting rounds |\n",
    "| | learning_rate | 0.01, 0.05, 0.1 | Step size, smaller values need more iterations but generalise better |\n",
    "| | max_depth | 5, 10, None | Tree depth per boosting round |\n",
    "| | min_samples_leaf | 10, 20, 30 | Leaf regularisation |\n",
    "| | l2_regularization | 0.0, 0.1, 1.0 | L2 penalty on leaf weights |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b4a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_results = {}\n",
    "\n",
    "# --- Ridge Tuning ---\n",
    "print(\"Tuning Ridge Regression...\")\n",
    "ridge_params = {'alpha': [0.1, 1.0, 10.0]}\n",
    "ridge_search = RandomizedSearchCV(\n",
    "    Ridge(random_state=42), ridge_params,\n",
    "    n_iter=3, cv=5, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1\n",
    ")\n",
    "ridge_search.fit(X_train_scaled, y_train)\n",
    "print(f\"  Best params: {ridge_search.best_params_}\")\n",
    "tuned_results['Ridge'] = {\n",
    "    'train': evaluate_model(y_train, ridge_search.predict(X_train_scaled), 'Train'),\n",
    "    'test': evaluate_model(y_test, ridge_search.predict(X_test_scaled), 'Test'),\n",
    "    'best_params': ridge_search.best_params_\n",
    "}\n",
    "\n",
    "print()\n",
    "\n",
    "# --- Random Forest Tuning ---\n",
    "print(\"Tuning Random Forest...\")\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "rf_search = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=42, n_jobs=-1), rf_params,\n",
    "    n_iter=25, cv=5, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_search.fit(X_train_imputed, y_train)\n",
    "print(f\"  Best params: {rf_search.best_params_}\")\n",
    "tuned_results['RandomForest'] = {\n",
    "    'train': evaluate_model(y_train, rf_search.predict(X_train_imputed), 'Train'),\n",
    "    'test': evaluate_model(y_test, rf_search.predict(X_test_imputed), 'Test'),\n",
    "    'best_params': rf_search.best_params_\n",
    "}\n",
    "\n",
    "print()\n",
    "\n",
    "# --- HistGradientBoosting Tuning ---\n",
    "print(\"Tuning HistGradientBoosting...\")\n",
    "hgb_params = {\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_leaf': [10, 20, 30],\n",
    "    'l2_regularization': [0.0, 0.1, 1.0]\n",
    "}\n",
    "hgb_search = RandomizedSearchCV(\n",
    "    HistGradientBoostingRegressor(random_state=42), hgb_params,\n",
    "    n_iter=25, cv=5, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1\n",
    ")\n",
    "hgb_search.fit(X_train, y_train)\n",
    "print(f\"  Best params: {hgb_search.best_params_}\")\n",
    "tuned_results['HistGBR'] = {\n",
    "    'train': evaluate_model(y_train, hgb_search.predict(X_train), 'Train'),\n",
    "    'test': evaluate_model(y_test, hgb_search.predict(X_test), 'Test'),\n",
    "    'best_params': hgb_search.best_params_\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c88a06e",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.1 Model Comparison - Before vs After Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c298de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison table\n",
    "print(\"Model Performance Comparison (Test Set, Dollar Scale)\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'':<20} {'R2':<10} {'RMSE':<15} {'MAE':<15} {'MAPE':<10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for label, res_dict in [('Before Tuning', results), ('After Tuning', tuned_results)]:\n",
    "    print(label)\n",
    "    for name, metrics in res_dict.items():\n",
    "        m = metrics['test']\n",
    "        print(f\"  {name:<18} {m['R2']:<10.4f} ${m['RMSE']:<14,.0f} ${m['MAE']:<14,.0f} {m['MAPE']:<10.1f}%\")\n",
    "\n",
    "# Identify best model\n",
    "best_name = max(tuned_results, key=lambda k: tuned_results[k]['test']['R2'])\n",
    "best_r2 = tuned_results[best_name]['test']['R2']\n",
    "print(f\"\\nBest model: {best_name} (R2 = {best_r2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b62a0",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.2 Residual Analysis\n",
    "\n",
    "Residual plots for the best model help check prediction quality across different price ranges. Ideally, residuals should be randomly scattered around zero with no pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1851dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best tuned model for residual analysis\n",
    "best_models = {'Ridge': ridge_search, 'RandomForest': rf_search, 'HistGBR': hgb_search}\n",
    "best_X_test = {'Ridge': X_test_scaled, 'RandomForest': X_test_imputed, 'HistGBR': X_test}\n",
    "\n",
    "best_model = best_models[best_name]\n",
    "y_pred_log = best_model.predict(best_X_test[best_name])\n",
    "y_pred_dollars = np.exp(y_pred_log)\n",
    "y_true_dollars = np.exp(y_test)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Predicted vs Actual\n",
    "axes[0].scatter(y_true_dollars, y_pred_dollars, alpha=0.15, s=5, color='steelblue')\n",
    "max_val = max(y_true_dollars.max(), y_pred_dollars.max())\n",
    "axes[0].plot([0, max_val], [0, max_val], 'r--', linewidth=1.5, label='Perfect prediction')\n",
    "axes[0].set_title(f'{best_name} -- Predicted vs Actual')\n",
    "axes[0].set_xlabel('Actual Price (USD)')\n",
    "axes[0].set_ylabel('Predicted Price (USD)')\n",
    "axes[0].xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'${x/1e6:.1f}M'))\n",
    "axes[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'${x/1e6:.1f}M'))\n",
    "axes[0].legend()\n",
    "\n",
    "# Residual distribution\n",
    "residuals = y_true_dollars - y_pred_dollars\n",
    "axes[1].hist(residuals, bins=80, color='steelblue', edgecolor='white', alpha=0.85)\n",
    "axes[1].axvline(x=0, color='red', linestyle='--')\n",
    "axes[1].set_title('Residual Distribution')\n",
    "axes[1].set_xlabel('Residual (Actual - Predicted, USD)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# Residuals vs Predicted\n",
    "axes[2].scatter(y_pred_dollars, residuals, alpha=0.15, s=5, color='steelblue')\n",
    "axes[2].axhline(y=0, color='red', linestyle='--')\n",
    "axes[2].set_title('Residuals vs Predicted')\n",
    "axes[2].set_xlabel('Predicted Price (USD)')\n",
    "axes[2].set_ylabel('Residual (USD)')\n",
    "axes[2].xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'${x/1e6:.1f}M'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7fe67a",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.3 Export Model\n",
    "\n",
    "Save model and setup lookup tables to deploy to Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b5859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create output directory for model artefacts\n",
    "os.makedirs('model', exist_ok=True)\n",
    "\n",
    "# 1. Save the best tuned Random Forest model (compressed to reduce file size)\n",
    "best_rf = rf_search.best_estimator_\n",
    "joblib.dump(best_rf, 'model/rf_model.joblib', compress=3)\n",
    "\n",
    "# 2. Save the feature column order (must match during inference)\n",
    "joblib.dump(X_train.columns.tolist(), 'model/feature_columns.joblib')\n",
    "\n",
    "# 3. Save imputation medians computed from training set (as plain dict for portability)\n",
    "joblib.dump(dict(impute_medians), 'model/impute_medians.joblib')\n",
    "\n",
    "# 4. Build consolidated lookup tables for the Streamlit app\n",
    "lookup_tables = {\n",
    "    # Frequency encoding tables\n",
    "    'borough_freq': df['BOROUGH'].value_counts(normalize=True).to_dict(),\n",
    "    'neighborhood_freq': df['NEIGHBORHOOD'].value_counts(normalize=True).to_dict(),\n",
    "    'zip_code_freq': df['ZIP CODE'].value_counts(normalize=True).to_dict(),\n",
    "    'building_class_category_freq': df['BUILDING CLASS CATEGORY'].value_counts(normalize=True).to_dict(),\n",
    "    'tax_class_at_present_freq': df['TAX CLASS AT PRESENT'].value_counts(normalize=True).to_dict(),\n",
    "    'tax_class_at_time_of_sale_freq': df['TAX CLASS AT TIME OF SALE'].value_counts(normalize=True).to_dict(),\n",
    "\n",
    "    # Median price tables (location quality proxies)\n",
    "    'neigh_median_price': df.groupby('NEIGHBORHOOD')['SALE PRICE'].median().to_dict(),\n",
    "    'zip_median_price': df.groupby('ZIP CODE')['SALE PRICE'].median().to_dict(),\n",
    "\n",
    "    # Dropdown option mappings (filtered by borough)\n",
    "    'borough_neighborhoods': df.groupby('BOROUGH')['NEIGHBORHOOD'].apply(\n",
    "        lambda x: sorted(x.unique().tolist())\n",
    "    ).to_dict(),\n",
    "    'borough_zipcodes': df.groupby('BOROUGH')['ZIP CODE'].apply(\n",
    "        lambda x: sorted(x.unique().tolist())\n",
    "    ).to_dict(),\n",
    "\n",
    "    # Dropdown option lists\n",
    "    'building_categories': sorted(df['BUILDING CLASS CATEGORY'].unique().tolist()),\n",
    "    'tax_class_at_present_options': sorted(df['TAX CLASS AT PRESENT'].unique().tolist()),\n",
    "    'tax_class_at_time_of_sale_options': sorted(\n",
    "        [int(x) if not isinstance(x, str) else x for x in df['TAX CLASS AT TIME OF SALE'].unique()]\n",
    "    ),\n",
    "\n",
    "    # Unit cap values (99th percentile from cleaned data)\n",
    "    'residential_units_cap': float(df['RESIDENTIAL UNITS'].max()),\n",
    "    'commercial_units_cap': float(df['COMMERCIAL UNITS'].max()),\n",
    "    'total_units_cap': float(df['TOTAL UNITS'].max()),\n",
    "\n",
    "    # Borough name mapping for display\n",
    "    'borough_map': {1: 'Manhattan', 2: 'Bronx', 3: 'Brooklyn', 4: 'Queens', 5: 'Staten Island'},\n",
    "}\n",
    "\n",
    "joblib.dump(lookup_tables, 'model/lookup_tables.joblib')\n",
    "\n",
    "print(\"Model artefacts saved to model/ directory:\")\n",
    "for f in sorted(os.listdir('model')):\n",
    "    size = os.path.getsize(f'model/{f}') / 1024\n",
    "    print(f\"  {f}: {size:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff4c200",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Conclusion\n",
    "\n",
    "### 6.1 Data Pipeline Summary\n",
    "\n",
    "| Stage | Detail |\n",
    "|---|---|\n",
    "| Raw data | 84,548 property transactions across 5 boroughs |\n",
    "| After cleaning | ~50K-54K rows (removing non-market sales, missing year built, etc.) |\n",
    "| Target transform | log(SALE PRICE) to normalise the right-skewed distribution |\n",
    "| Outlier treatment | 99th percentile cap on sale price |\n",
    "| Missing data | YEAR BUILT=0 removed; GROSS/LAND SQFT=0 treated as NaN; HistGBR uses native NaN handling, RF/Ridge use median imputation |\n",
    "| Features | ~25 numeric features including frequency-encoded categoricals, location aggregations, building age, and binary indicators |\n",
    "\n",
    "### 6.2 Model Performance\n",
    "\n",
    "All metrics are computed on the dollar scale (after inverse log-transform) on the 20% test set.\n",
    "\n",
    "The best performing model is determined by the highest R-squared on the test set. Ridge Regression, performs worse than the tree-based models due to it being unable to capture non-linear relationships. Random Forest and HistGradientBoosting both achieve substantially higher R-squared values by learning complex interactions between location, size, and building characteristics.\n",
    "\n",
    "### 6.3 Findings\n",
    "\n",
    "1. **Location dominates pricing:** Neighbourhood median price and borough frequency encoding are consistently the most important features across both tree-based models, confirming that NYC property prices is driven primarily by location.\n",
    "\n",
    "2. **Log transformation is critical:** Without it, all models produced negative R-squared values (worse than predicting the mean), due to the extreme skew in sale prices.\n",
    "\n",
    "3. **Tree-based models significantly outperform linear:** The non-linear nature of real estate pricing (where the same square footage commands vastly different prices depending on borough, building type, and age) makes tree-based ensembles far more suitable than Ridge Regression.\n",
    "\n",
    "4. **Native NaN handling by Hist regressor:** HistGradientBoosting's native ability to handle missing square footage data, rather than relying on median imputation, allows it to learn different patterns for properties with and without reported square footage.\n",
    "\n",
    "5. **Hyperparameter tuning provides so-so gains:** RandomizedSearchCV improved performance marginally over the initial configurations. This shows that the default hyperparameters were already decent.\n",
    "\n",
    "### 6.4 Limitations\n",
    "\n",
    "- **Time scope:** The dataset covers only 2016-2017; market conditions change over time and the model would need retraining for current predictions\n",
    "- **Missing square footage:** ~47% of properties lack gross square footage data, limiting the model's ability to use the strongest size predictor\n",
    "- **Feature leakage:** Location-based price aggregations (neigh_median_price) are computed on the full dataset; a production system should compute these only from training data\n",
    "- **Price cap at 99th percentile:** The model cannot accurately predict ultra-luxury properties above the cap threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "print(\"Final Model Rankings (Test Set)\")\n",
    "print(\"-\" * 60)\n",
    "all_results = {**{k + ' (base)': v for k, v in results.items()},\n",
    "               **{k + ' (tuned)': v for k, v in tuned_results.items()}}\n",
    "\n",
    "ranking = sorted(all_results.items(), key=lambda x: x[1]['test']['R2'], reverse=True)\n",
    "for i, (name, metrics) in enumerate(ranking, 1):\n",
    "    m = metrics['test']\n",
    "    print(f\"  {i}. {name:<25} R2={m['R2']:.4f}  RMSE=${m['RMSE']:>12,.0f}  MAE=${m['MAE']:>12,.0f}  MAPE={m['MAPE']:.1f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
